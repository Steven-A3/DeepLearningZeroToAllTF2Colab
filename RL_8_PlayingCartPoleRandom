{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"RL_8_PlayingCartPoleRandom","provenance":[{"file_id":"1flu31ulJlgiRL1dnN2ir8wGh9p7Zij2t","timestamp":1575462605583},{"file_id":"1A75J2xFYjpJvNWXCSM1Xcty7K3WIGrud","timestamp":1542848369662}],"private_outputs":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"odNaDE1zyrL2","colab_type":"text"},"source":["# install dependancies, takes around 45 seconds\n","\n","Rendering Dependancies\n","\n"]},{"cell_type":"code","metadata":{"id":"8-AxnvAVyzQQ","colab_type":"code","colab":{}},"source":["#remove \" > /dev/null 2>&1\" to see what is going on under the hood\n","!pip install gym pyvirtualdisplay > /dev/null 2>&1\n","!apt-get install -y xvfb python-opengl ffmpeg > /dev/null 2>&1"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8A-1LTSH88EE","colab_type":"text"},"source":["CartPole Dependancies"]},{"cell_type":"code","metadata":{"id":"TCelFzWY9MBI","colab_type":"code","colab":{}},"source":["!apt-get update > /dev/null 2>&1\n","!apt-get install cmake > /dev/null 2>&1\n","!pip install --upgrade setuptools 2>&1\n","!pip install ez_setup > /dev/null 2>&1\n","!pip install gym[atari] > /dev/null 2>&1"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"APXSx7hg19TH","colab_type":"text"},"source":["# Imports and Helper functions\n"]},{"cell_type":"code","metadata":{"id":"pdb2JwZy4jGj","colab_type":"code","colab":{}},"source":["import gym\n","from gym import logger as gymlogger\n","from gym.wrappers import Monitor\n","gymlogger.set_level(40) #error only\n","import tensorflow as tf\n","import numpy as np\n","import random\n","import matplotlib\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","import math\n","import glob\n","import io\n","import base64\n","from IPython.display import HTML\n","\n","from IPython import display as ipythondisplay"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"nQEtc28G4niA","colab_type":"code","colab":{}},"source":["from pyvirtualdisplay import Display\n","display = Display(visible=0, size=(1400, 900))\n","display.start()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"G9UWeToN4r7D","colab_type":"code","colab":{}},"source":["\"\"\"\n","Utility functions to enable video recording of gym environment and displaying it\n","To enable video, just do \"env = wrap_env(env)\"\"\n","\"\"\"\n","\n","def show_video():\n","  mp4list = glob.glob('video/*.mp4')\n","  if len(mp4list) > 0:\n","    mp4 = mp4list[0]\n","    video = io.open(mp4, 'r+b').read()\n","    encoded = base64.b64encode(video)\n","    ipythondisplay.display(HTML(data='''<video alt=\"test\" autoplay \n","                loop controls style=\"height: 400px;\">\n","                <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n","             </video>'''.format(encoded.decode('ascii'))))\n","  else: \n","    print(\"Could not find video\")\n","    \n","\n","def wrap_env(env):\n","  env = Monitor(env, './video', force=True)\n","  return env"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"W3BGbWOu179M","colab_type":"text"},"source":["# CartPole!"]},{"cell_type":"code","metadata":{"id":"dGEFMfDOzLen","colab_type":"code","colab":{}},"source":["env = wrap_env(gym.make(\"CartPole-v0\"))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"7BmIlXhe9Q89","colab_type":"code","colab":{}},"source":["#check out the CartPole action space!\n","print(env.action_space)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"8nj5sjsk15IT","colab_type":"code","colab":{}},"source":["env.reset()\n","random_episodes = 0\n","reward_sum = 0\n","while random_episodes < 10:\n","    env.render()\n","    action = env.action_space.sample()\n","    observation, reward, done, _ = env.step(action)\n","    print(observation, reward, done)\n","    reward_sum += reward\n","    if done:\n","        random_episodes += 1\n","        print(\"Reward for this episode was:\", reward_sum)\n","        reward_sum = 0\n","        env.reset()\n","        \n","show_video()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"uynQrxJyj3RZ","colab_type":"code","colab":{}},"source":["# Constants defining our neural network\n","learning_rate = 1e-1\n","input_size = env.observation_space.shape[0]\n","output_size = env.action_space.n\n","\n","X = tf.placeholder(tf.float32, [None, input_size], name=\"input_x\")\n","\n","# First layer of weights\n","W1 = tf.get_variable(\"W1\", shape=[input_size, output_size],\n","                     initializer=tf.contrib.layers.xavier_initializer())\n","Qpred = tf.matmul(X, W1)\n","\n","# We need to define the parts of the network needed for learning a policy\n","Y = tf.placeholder(shape=[None, output_size], dtype=tf.float32)\n","\n","# Loss function\n","loss = tf.reduce_sum(tf.square(Y - Qpred))\n","# Learning\n","train = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(loss)\n","\n","# Values for q learning\n","max_episodes = 5000\n","dis = 0.9\n","step_history = []\n","\n","\n","# Setting up our environment\n","init = tf.global_variables_initializer()\n","sess = tf.Session()\n","sess.run(init)\n","\n","for episode in range(max_episodes):\n","    e = 1. / ((episode / 10) + 1)\n","    step_count = 0\n","    state = env.reset()\n","    done = False\n","\n","    # The Q-Network training\n","    while not done:\n","        step_count += 1\n","        x = np.reshape(state, [1, input_size])\n","        # Choose an action by greedily (with e chance of random action) from\n","        # the Q-network\n","        Q = sess.run(Qpred, feed_dict={X: x})\n","        if np.random.rand(1) < e:\n","            action = env.action_space.sample()\n","        else:\n","            action = np.argmax(Q)\n","\n","        # Get new state and reward from environment\n","        next_state, reward, done, _ = env.step(action)\n","        if done:\n","            Q[0, action] = -100\n","        else:\n","            x_next = np.reshape(next_state, [1, input_size])\n","            # Obtain the Q' values by feeding the new state through our network\n","            Q_next = sess.run(Qpred, feed_dict={X: x_next})\n","            Q[0, action] = reward + dis * np.max(Q_next)\n","\n","        # Train our network using target and predicted Q values on each episode\n","        sess.run(train, feed_dict={X: x, Y: Q})\n","        state = next_state\n","\n","    step_history.append(step_count)\n","    print(\"Episode: {}  steps: {}\".format(episode, step_count))\n","    # If last 10's avg steps are 500, it's good enough\n","    if len(step_history) > 10 and np.mean(step_history[-10:]) > 500:\n","        break\n","\n","# See our trained network in action\n","observation = env.reset()\n","reward_sum = 0\n","while True:\n","    env.render()\n","\n","    x = np.reshape(observation, [1, input_size])\n","    Q = sess.run(Qpred, feed_dict={X: x})\n","    action = np.argmax(Q)\n","\n","    observation, reward, done, _ = env.step(action)\n","    reward_sum += reward\n","    if done:\n","        print(\"Total score: {}\".format(reward_sum))\n","        break\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"kpJTWRkcn_VL","colab_type":"code","colab":{}},"source":["class DQN:\n","\n","    def __init__(self, session: tf.Session, input_size: int, output_size: int, name: str=\"main\") -> None:\n","        \"\"\"DQN Agent can\n","\n","        1) Build network\n","        2) Predict Q_value given state\n","        3) Train parameters\n","\n","        Args:\n","            session (tf.Session): Tensorflow session\n","            input_size (int): Input dimension\n","            output_size (int): Number of discrete actions\n","            name (str, optional): TF Graph will be built under this name scope\n","        \"\"\"\n","        self.session = session\n","        self.input_size = input_size\n","        self.output_size = output_size\n","        self.net_name = name\n","\n","        self._build_network()\n","\n","    def _build_network(self, h_size=16, l_rate=0.001) -> None:\n","        \"\"\"DQN Network architecture (simple MLP)\n","\n","        Args:\n","            h_size (int, optional): Hidden layer dimension\n","            l_rate (float, optional): Learning rate\n","        \"\"\"\n","        with tf.variable_scope(self.net_name):\n","            self._X = tf.placeholder(tf.float32, [None, self.input_size], name=\"input_x\")\n","            net = self._X\n","\n","            net = tf.layers.dense(net, h_size, activation=tf.nn.relu)\n","            net = tf.layers.dense(net, self.output_size)\n","            self._Qpred = net\n","\n","            self._Y = tf.placeholder(tf.float32, shape=[None, self.output_size])\n","            self._loss = tf.losses.mean_squared_error(self._Y, self._Qpred)\n","\n","            optimizer = tf.train.AdamOptimizer(learning_rate=l_rate)\n","            self._train = optimizer.minimize(self._loss)\n","\n","    def predict(self, state: np.ndarray) -> np.ndarray:\n","        \"\"\"Returns Q(s, a)\n","\n","        Args:\n","            state (np.ndarray): State array, shape (n, input_dim)\n","\n","        Returns:\n","            np.ndarray: Q value array, shape (n, output_dim)\n","        \"\"\"\n","        x = np.reshape(state, [-1, self.input_size])\n","        return self.session.run(self._Qpred, feed_dict={self._X: x})\n","\n","    def update(self, x_stack: np.ndarray, y_stack: np.ndarray) -> list:\n","        \"\"\"Performs updates on given X and y and returns a result\n","\n","        Args:\n","            x_stack (np.ndarray): State array, shape (n, input_dim)\n","            y_stack (np.ndarray): Target Q array, shape (n, output_dim)\n","\n","        Returns:\n","            list: First element is loss, second element is a result from train step\n","        \"\"\"\n","        feed = {\n","            self._X: x_stack,\n","            self._Y: y_stack\n","        }\n","        return self.session.run([self._loss, self._train], feed)\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"j6UjvqHflAzh","colab_type":"code","colab":{}},"source":["INPUT_SIZE = env.observation_space.shape[0]\n","OUTPUT_SIZE = env.action_space.n\n","\n","DISCOUNT_RATE = 0.99\n","REPLAY_MEMORY = 50000\n","MAX_EPISODE = 5000\n","BATCH_SIZE = 64\n","\n","# minimum epsilon for epsilon greedy\n","MIN_E = 0.0\n","# epsilon will be `MIN_E` at `EPSILON_DECAYING_EPISODE`\n","EPSILON_DECAYING_EPISODE = MAX_EPISODE * 0.01\n","\n","\n","def bot_play(mainDQN: DQN) -> None:\n","    \"\"\"Runs a single episode with rendering and prints a reward\n","\n","    Args:\n","        mainDQN (dqn.DQN): DQN Agent\n","    \"\"\"\n","    state = env.reset()\n","    total_reward = 0\n","\n","    while True:\n","        env.render()\n","        action = np.argmax(mainDQN.predict(state))\n","        state, reward, done, _ = env.step(action)\n","        total_reward += reward\n","        if done:\n","            print(\"Total score: {}\".format(total_reward))\n","            break\n","\n","\n","def train_minibatch(DQN: DQN, train_batch: list) -> float:\n","    \"\"\"Prepare X_batch, y_batch and train them\n","\n","    Recall our loss function is\n","        target = reward + discount * max Q(s',a)\n","                 or reward if done early\n","\n","        Loss function: [target - Q(s, a)]^2\n","\n","    Hence,\n","\n","        X_batch is a state list\n","        y_batch is reward + discount * max Q\n","                   or reward if terminated early\n","\n","    Args:\n","        DQN (dqn.DQN): DQN Agent to train & run\n","        train_batch (list): Minibatch of Replay memory\n","            Eeach element is a tuple of (s, a, r, s', done)\n","\n","    Returns:\n","        loss: Returns a loss\n","\n","    \"\"\"\n","    state_array = np.vstack([x[0] for x in train_batch])\n","    action_array = np.array([x[1] for x in train_batch])\n","    reward_array = np.array([x[2] for x in train_batch])\n","    next_state_array = np.vstack([x[3] for x in train_batch])\n","    done_array = np.array([x[4] for x in train_batch])\n","\n","    X_batch = state_array\n","    y_batch = DQN.predict(state_array)\n","\n","    Q_target = reward_array + DISCOUNT_RATE * np.max(DQN.predict(next_state_array), axis=1) * ~done_array\n","    y_batch[np.arange(len(X_batch)), action_array] = Q_target\n","\n","    # Train our network using target and predicted Q values on each episode\n","    loss, _ = DQN.update(X_batch, y_batch)\n","\n","    return loss\n","\n","\n","def annealing_epsilon(episode: int, min_e: float, max_e: float, target_episode: int) -> float:\n","    \"\"\"Return an linearly annealed epsilon\n","\n","    Epsilon will decrease over time until it reaches `target_episode`\n","\n","         (epsilon)\n","             |\n","    max_e ---|\\\n","             | \\\n","             |  \\\n","             |   \\\n","    min_e ---|____\\_______________(episode)\n","                  |\n","                 target_episode\n","\n","     slope = (min_e - max_e) / (target_episode)\n","     intercept = max_e\n","\n","     e = slope * episode + intercept\n","\n","    Args:\n","        episode (int): Current episode\n","        min_e (float): Minimum epsilon\n","        max_e (float): Maximum epsilon\n","        target_episode (int): epsilon becomes the `min_e` at `target_episode`\n","\n","    Returns:\n","        float: epsilon between `min_e` and `max_e`\n","    \"\"\"\n","\n","    slope = (min_e - max_e) / (target_episode)\n","    intercept = max_e\n","\n","    return max(min_e, slope * episode + intercept)\n","\n","\n","  "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"EqpBan_xnY_1","colab_type":"code","colab":{}},"source":["from collections import deque\n","\n","# store the previous observations in replay memory\n","replay_buffer = deque(maxlen=REPLAY_MEMORY)\n","last_100_game_reward = deque(maxlen=100)\n","\n","with tf.Session() as sess:\n","    mainDQN = DQN(sess, INPUT_SIZE, OUTPUT_SIZE)\n","    init = tf.global_variables_initializer()\n","    sess.run(init)\n","\n","    for episode in range(MAX_EPISODE):\n","        e = annealing_epsilon(episode, MIN_E, 1.0, EPSILON_DECAYING_EPISODE)\n","        done = False\n","        state = env.reset()\n","\n","        step_count = 0\n","        while not done:\n","\n","            if np.random.rand() < e:\n","                action = env.action_space.sample()\n","            else:\n","                action = np.argmax(mainDQN.predict(state))\n","\n","            next_state, reward, done, _ = env.step(action)\n","\n","            if done:\n","                reward = -1\n","\n","            replay_buffer.append((state, action, reward, next_state, done))\n","\n","            state = next_state\n","            step_count += 1\n","\n","            if len(replay_buffer) > BATCH_SIZE:\n","                minibatch = random.sample(replay_buffer, BATCH_SIZE)\n","                train_minibatch(mainDQN, minibatch)\n","\n","        print(\"[Episode {:>5}]  steps: {:>5} e: {:>5.2f}\".format(episode, step_count, e))\n","\n","        # CartPole-v0 Game Clear Logic\n","        last_100_game_reward.append(step_count)\n","        if len(last_100_game_reward) == last_100_game_reward.maxlen:\n","            avg_reward = np.mean(last_100_game_reward)\n","            if avg_reward > 199.0:\n","                print(\"Game Cleared within {} episodes with avg reward {}\".format(episode, avg_reward))\n","                break\n"],"execution_count":0,"outputs":[]}]}