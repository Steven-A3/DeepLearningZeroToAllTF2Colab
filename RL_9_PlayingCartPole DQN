{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"RL_9_PlayingCartPole DQN","provenance":[{"file_id":"1flu31ulJlgiRL1dnN2ir8wGh9p7Zij2t","timestamp":1575462605583},{"file_id":"1A75J2xFYjpJvNWXCSM1Xcty7K3WIGrud","timestamp":1542848369662}],"private_outputs":true,"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"TPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"odNaDE1zyrL2","colab_type":"text"},"source":["# install dependancies, takes around 45 seconds\n","\n","Rendering Dependancies\n","\n"]},{"cell_type":"code","metadata":{"id":"8-AxnvAVyzQQ","colab_type":"code","colab":{}},"source":["#remove \" > /dev/null 2>&1\" to see what is going on under the hood\n","!pip install gym pyvirtualdisplay > /dev/null 2>&1\n","!apt-get install -y xvfb python-opengl ffmpeg > /dev/null 2>&1"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8A-1LTSH88EE","colab_type":"text"},"source":["CartPole Dependancies"]},{"cell_type":"code","metadata":{"id":"TCelFzWY9MBI","colab_type":"code","colab":{}},"source":["!apt-get update > /dev/null 2>&1\n","!apt-get install cmake > /dev/null 2>&1\n","!pip install --upgrade setuptools 2>&1\n","!pip install ez_setup > /dev/null 2>&1\n","!pip install gym[atari] > /dev/null 2>&1"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"APXSx7hg19TH","colab_type":"text"},"source":["# Imports and Helper functions\n"]},{"cell_type":"code","metadata":{"id":"pdb2JwZy4jGj","colab_type":"code","colab":{}},"source":["import gym\n","from gym import logger as gymlogger\n","from gym.wrappers import Monitor\n","gymlogger.set_level(40) #error only\n","import tensorflow as tf\n","import numpy as np\n","import random\n","import matplotlib\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","import math\n","import glob\n","import io\n","import base64\n","from IPython.display import HTML\n","\n","from IPython import display as ipythondisplay"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"nQEtc28G4niA","colab_type":"code","colab":{}},"source":["from pyvirtualdisplay import Display\n","display = Display(visible=0, size=(1400, 900))\n","display.start()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"G9UWeToN4r7D","colab_type":"code","colab":{}},"source":["\"\"\"\n","Utility functions to enable video recording of gym environment and displaying it\n","To enable video, just do \"env = wrap_env(env)\"\"\n","\"\"\"\n","\n","def show_video():\n","  mp4list = glob.glob('video/*.mp4')\n","  if len(mp4list) > 0:\n","    mp4 = mp4list[0]\n","    video = io.open(mp4, 'r+b').read()\n","    encoded = base64.b64encode(video)\n","    ipythondisplay.display(HTML(data='''<video alt=\"test\" autoplay \n","                loop controls style=\"height: 400px;\">\n","                <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n","             </video>'''.format(encoded.decode('ascii'))))\n","  else: \n","    print(\"Could not find video\")\n","    \n","\n","def wrap_env(env):\n","  env = Monitor(env, './video', force=True)\n","  return env"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"W3BGbWOu179M","colab_type":"text"},"source":["# CartPole!"]},{"cell_type":"code","metadata":{"id":"dGEFMfDOzLen","colab_type":"code","colab":{}},"source":["from gym.envs.registration import register\n","\n","register(\n","    id='CartPole-v10',\n","    entry_point='gym.envs.classic_control:CartPoleEnv',\n","    max_episode_steps=10000,\n","    reward_threshold=195.0,\n",")\n","\n","env = wrap_env(gym.make(\"CartPole-v10\"))\n","env = gym.wrappers.Monitor(env, directory=\"gym-results/\", force=True)\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"7BmIlXhe9Q89","colab_type":"code","colab":{}},"source":["#check out the CartPole action space!\n","print(env.action_space)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"kpJTWRkcn_VL","colab_type":"code","colab":{}},"source":["from collections import deque\n","from typing import List\n","\n","class DQN:\n","\n","    def __init__(self, session: tf.Session, input_size: int, output_size: int, name: str=\"main\") -> None:\n","        \"\"\"DQN Agent can\n","\n","        1) Build network\n","        2) Predict Q_value given state\n","        3) Train parameters\n","\n","        Args:\n","            session (tf.Session): Tensorflow session\n","            input_size (int): Input dimension\n","            output_size (int): Number of discrete actions\n","            name (str, optional): TF Graph will be built under this name scope\n","        \"\"\"\n","        self.session = session\n","        self.input_size = input_size\n","        self.output_size = output_size\n","        self.net_name = name\n","\n","        self._build_network()\n","\n","    def _build_network(self, h_size=16, l_rate=0.001) -> None:\n","        \"\"\"DQN Network architecture (simple MLP)\n","\n","        Args:\n","            h_size (int, optional): Hidden layer dimension\n","            l_rate (float, optional): Learning rate\n","        \"\"\"\n","        with tf.variable_scope(self.net_name):\n","            self._X = tf.placeholder(tf.float32, [None, self.input_size], name=\"input_x\")\n","            net = self._X\n","\n","            net = tf.layers.dense(net, h_size, activation=tf.nn.relu)\n","            net = tf.layers.dense(net, self.output_size)\n","            self._Qpred = net\n","\n","            self._Y = tf.placeholder(tf.float32, shape=[None, self.output_size])\n","            self._loss = tf.losses.mean_squared_error(self._Y, self._Qpred)\n","\n","            optimizer = tf.train.AdamOptimizer(learning_rate=l_rate)\n","            self._train = optimizer.minimize(self._loss)\n","\n","    def predict(self, state: np.ndarray) -> np.ndarray:\n","        \"\"\"Returns Q(s, a)\n","\n","        Args:\n","            state (np.ndarray): State array, shape (n, input_dim)\n","\n","        Returns:\n","            np.ndarray: Q value array, shape (n, output_dim)\n","        \"\"\"\n","        x = np.reshape(state, [-1, self.input_size])\n","        return self.session.run(self._Qpred, feed_dict={self._X: x})\n","\n","    def update(self, x_stack: np.ndarray, y_stack: np.ndarray) -> list:\n","        \"\"\"Performs updates on given X and y and returns a result\n","\n","        Args:\n","            x_stack (np.ndarray): State array, shape (n, input_dim)\n","            y_stack (np.ndarray): Target Q array, shape (n, output_dim)\n","\n","        Returns:\n","            list: First element is loss, second element is a result from train step\n","        \"\"\"\n","        feed = {\n","            self._X: x_stack,\n","            self._Y: y_stack\n","        }\n","        return self.session.run([self._loss, self._train], feed)\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"4PYDo6gK9yMn","colab_type":"code","colab":{}},"source":["# Constants defining our neural network\n","INPUT_SIZE = env.observation_space.shape[0]\n","OUTPUT_SIZE = env.action_space.n\n","\n","DISCOUNT_RATE = 0.99\n","REPLAY_MEMORY = 50000\n","BATCH_SIZE = 64\n","TARGET_UPDATE_FREQUENCY = 5\n","MAX_EPISODES = 5000\n","\n","\n","def replay_train(mainDQN: DQN, targetDQN: DQN, train_batch: list) -> float:\n","    \"\"\"Trains `mainDQN` with target Q values given by `targetDQN`\n","\n","    Args:\n","        mainDQN (dqn.DQN): Main DQN that will be trained\n","        targetDQN (dqn.DQN): Target DQN that will predict Q_target\n","        train_batch (list): Minibatch of replay memory\n","            Each element is (s, a, r, s', done)\n","            [(state, action, reward, next_state, done), ...]\n","\n","    Returns:\n","        float: After updating `mainDQN`, it returns a `loss`\n","    \"\"\"\n","    states = np.vstack([x[0] for x in train_batch])\n","    actions = np.array([x[1] for x in train_batch])\n","    rewards = np.array([x[2] for x in train_batch])\n","    next_states = np.vstack([x[3] for x in train_batch])\n","    done = np.array([x[4] for x in train_batch])\n","\n","    X = states\n","\n","    Q_target = rewards + DISCOUNT_RATE * np.max(targetDQN.predict(next_states), axis=1) * ~done\n","\n","    y = mainDQN.predict(states)\n","    y[np.arange(len(X)), actions] = Q_target\n","\n","    # Train our network using target and predicted Q values on each episode\n","    return mainDQN.update(X, y)\n","\n","\n","def get_copy_var_ops(*, dest_scope_name: str, src_scope_name: str) -> List[tf.Operation]:\n","    \"\"\"Creates TF operations that copy weights from `src_scope` to `dest_scope`\n","\n","    Args:\n","        dest_scope_name (str): Destination weights (copy to)\n","        src_scope_name (str): Source weight (copy from)\n","\n","    Returns:\n","        List[tf.Operation]: Update operations are created and returned\n","    \"\"\"\n","    # Copy variables src_scope to dest_scope\n","    op_holder = []\n","\n","    src_vars = tf.get_collection(\n","        tf.GraphKeys.TRAINABLE_VARIABLES, scope=src_scope_name)\n","    dest_vars = tf.get_collection(\n","        tf.GraphKeys.TRAINABLE_VARIABLES, scope=dest_scope_name)\n","\n","    for src_var, dest_var in zip(src_vars, dest_vars):\n","        op_holder.append(dest_var.assign(src_var.value()))\n","\n","    return op_holder\n","\n","\n","def bot_play(mainDQN: DQN, env: gym.Env) -> None:\n","    \"\"\"Test runs with rendering and prints the total score\n","\n","    Args:\n","        mainDQN (dqn.DQN): DQN agent to run a test\n","        env (gym.Env): Gym Environment\n","    \"\"\"\n","    state = env.reset()\n","    reward_sum = 0\n","\n","    while True:\n","\n","        env.render()\n","        action = np.argmax(mainDQN.predict(state))\n","        state, reward, done, _ = env.step(action)\n","        reward_sum += reward\n","\n","        if done:\n","            print(\"Total score: {}\".format(reward_sum))\n","            break\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"7C-6MGLA9zEJ","colab_type":"code","colab":{}},"source":["# store the previous observations in replay memory\n","replay_buffer = deque(maxlen=REPLAY_MEMORY)\n","\n","last_100_game_reward = deque(maxlen=100)\n","\n","with tf.Session() as sess:\n","    mainDQN = DQN(sess, INPUT_SIZE, OUTPUT_SIZE, name=\"main\")\n","    targetDQN = DQN(sess, INPUT_SIZE, OUTPUT_SIZE, name=\"target\")\n","    sess.run(tf.global_variables_initializer())\n","\n","    # initial copy q_net -> target_net\n","    copy_ops = get_copy_var_ops(dest_scope_name=\"target\",\n","                                src_scope_name=\"main\")\n","    sess.run(copy_ops)\n","\n","    for episode in range(MAX_EPISODES):\n","        e = 1. / ((episode / 10) + 1)\n","        done = False\n","        step_count = 0\n","        state = env.reset()\n","\n","        while not done:\n","            if np.random.rand() < e:\n","                action = env.action_space.sample()\n","            else:\n","                # Choose an action by greedily from the Q-network\n","                action = np.argmax(mainDQN.predict(state))\n","\n","            # Get new state and reward from environment\n","            next_state, reward, done, _ = env.step(action)\n","\n","            if done:  # Penalty\n","                reward = -1\n","\n","            # Save the experience to our buffer\n","            replay_buffer.append((state, action, reward, next_state, done))\n","\n","            if len(replay_buffer) > BATCH_SIZE:\n","                minibatch = random.sample(replay_buffer, BATCH_SIZE)\n","                loss, _ = replay_train(mainDQN, targetDQN, minibatch)\n","\n","            if step_count % TARGET_UPDATE_FREQUENCY == 0:\n","                sess.run(copy_ops)\n","\n","            state = next_state\n","            step_count += 1\n","\n","        print(\"Episode: {}  steps: {}\".format(episode, step_count))\n","\n","        # CartPole-v0 Game Clear Checking Logic\n","        last_100_game_reward.append(step_count)\n","\n","        if len(last_100_game_reward) == last_100_game_reward.maxlen:\n","            avg_reward = np.mean(last_100_game_reward)\n","\n","            if avg_reward > 199:\n","                print(f\"Game Cleared in {episode} episodes with avg reward {avg_reward}\")\n","                break\n"],"execution_count":0,"outputs":[]}]}